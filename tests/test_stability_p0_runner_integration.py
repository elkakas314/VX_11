"""
Integration tests for VX11 Stability P0 Runner.

Only runs if VX11_INTEGRATION=1 environment variable is set.
Requires docker to be available.
"""

import json
import os
import pytest
from pathlib import Path
import sys

# Only run if VX11_INTEGRATION is set
pytestmark = pytest.mark.skipif(
    os.getenv("VX11_INTEGRATION") != "1",
    reason="Set VX11_INTEGRATION=1 to enable integration tests",
)

# Add parent to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from scripts.vx11_stability_p0 import (
    StabilityP0Runner,
    MODULE_DEPENDENCY_MAP,
    health_check,
    docker_stats,
    docker_inspect,
)


@pytest.mark.integration
@pytest.mark.stability
@pytest.mark.docker
class TestStabilityP0RunnerIntegration:
    """Integration tests with real docker services."""

    def test_health_check_endpoint(self):
        """Test health_check with a real endpoint (if available)."""
        # Try localhost madre endpoint; should fail if service not running
        success, latency_ms = health_check(
            "http://127.0.0.1:8000", timeout=2, max_retries=1
        )

        # We don't assert success (service may not be running)
        # Just verify that health_check returns reasonable values
        assert isinstance(success, bool)
        assert latency_ms >= 0

    def test_docker_stats_nonexistent_container(self):
        """Test docker_stats with non-existent container."""
        stats = docker_stats("nonexistent-container-xyz")
        # Should return empty dict or handle gracefully
        assert isinstance(stats, dict)

    def test_docker_inspect_nonexistent_container(self):
        """Test docker_inspect with non-existent container."""
        inspect = docker_inspect("nonexistent-container-xyz")
        # Should return empty dict or handle gracefully
        assert isinstance(inspect, dict)


@pytest.mark.integration
@pytest.mark.stability
@pytest.mark.docker
class TestStabilityP0RunnerCycle:
    """Test a short stability cycle."""

    def test_single_cycle_short_duration(self):
        """Run a minimal stability cycle (baseline only)."""
        runner = StabilityP0Runner(
            mode="low_power",
            cycles=1,
            modules="",  # Emptyâ€”just baseline
            timeout_sec=10,
            keep_core=True,
        )

        # Skip actual run to avoid Docker setup complexity
        # Just verify runner can be instantiated
        assert runner.audit_dir.exists()
        assert (runner.audit_dir / "raw").exists()

    def test_report_generation(self):
        """Test that reports are generated correctly."""
        runner = StabilityP0Runner(
            cycles=1,
            timeout_sec=10,
        )

        # Add a mock module result
        runner.results["modules"]["test-module"] = {
            "name": "test-module",
            "status": "PASS",
            "metrics": {
                "test-svc": {
                    "stats": {"memory_usage": "100MB"},
                    "inspect": {"restart_count": 0, "oom_killed": False},
                    "health_latency_ms": [50.0],
                }
            },
            "test_results": {},
            "errors": [],
        }
        runner.results["modules_tested"].append("test-module")
        runner.results["summary"]["total_modules"] = 1
        runner.results["summary"]["passed"] = 1

        runner._generate_reports()

        # Verify JSON report
        json_file = runner.audit_dir / "REPORT.json"
        assert json_file.exists()
        data = json.loads(json_file.read_text())
        assert data["summary"]["total_modules"] == 1
        assert data["summary"]["passed"] == 1

        # Verify Markdown report
        md_file = runner.audit_dir / "REPORT.md"
        assert md_file.exists()
        md_content = md_file.read_text()
        assert "test-module" in md_content
        assert "âœ…" in md_content or "PASS" in md_content

    def test_metrics_collection_format(self):
        """Test that metrics are collected in correct format."""
        runner = StabilityP0Runner()

        # Manually set a module with metrics
        runner.results["modules"]["metrics-test"] = {
            "name": "metrics-test",
            "status": "PASS",
            "metrics": {
                "vx11-service": {
                    "stats": {
                        "container_name": "vx11-service",
                        "status": "running",
                        "running": True,
                        "restart_count": 0,
                        "oom_killed": False,
                    },
                    "inspect": {
                        "container_id": "abc123",
                        "state": "running",
                        "running": True,
                        "restart_count": 0,
                        "oom_killed": False,
                    },
                    "health_latency_ms": [45.5, 48.2, 46.1],
                }
            },
            "test_results": {},
            "errors": [],
        }

        # Verify structure is valid JSON-serializable
        json_str = json.dumps(runner.results)
        data = json.loads(json_str)

        # Verify metrics structure
        metrics = data["modules"]["metrics-test"]["metrics"]["vx11-service"]
        assert metrics["stats"]["restart_count"] == 0
        assert metrics["stats"]["oom_killed"] == False
        assert len(metrics["health_latency_ms"]) == 3


@pytest.mark.integration
@pytest.mark.stability
class TestStabilityThresholds:
    """Test stability thresholds and failure detection."""

    def test_oom_killed_detection(self):
        """Test OOM killed detection in metrics."""
        runner = StabilityP0Runner()

        # Add a module with OOMKilled set
        runner.results["modules"]["oom-test"] = {
            "name": "oom-test",
            "status": "FAIL",
            "metrics": {
                "crashed-svc": {
                    "stats": {},
                    "inspect": {"oom_killed": True, "restart_count": 1},
                    "health_latency_ms": [],
                }
            },
            "test_results": {},
            "errors": ["crashed-svc was OOMKilled"],
        }

        json_str = json.dumps(runner.results)
        data = json.loads(json_str)

        # Verify OOM is captured
        metrics = data["modules"]["oom-test"]["metrics"]["crashed-svc"]
        assert metrics["inspect"]["oom_killed"] == True

    def test_high_latency_detection(self):
        """Test detection of high latencies."""
        runner = StabilityP0Runner()

        # Add metrics with high latencies
        runner.results["modules"]["latency-test"] = {
            "name": "latency-test",
            "status": "WARN",
            "metrics": {
                "slow-svc": {
                    "stats": {},
                    "inspect": {"restart_count": 0, "oom_killed": False},
                    "health_latency_ms": [450.0, 480.0, 520.0, 490.0, 510.0],
                }
            },
            "test_results": {},
            "errors": [],
        }

        # Calculate p95 latency
        latencies = runner.results["modules"]["latency-test"]["metrics"]["slow-svc"][
            "health_latency_ms"
        ]
        p95 = sorted(latencies)[int(len(latencies) * 0.95)]

        assert p95 > 500.0, "p95 latency should exceed 500ms threshold"

    def test_restart_count_tracking(self):
        """Test restart count in metrics."""
        runner = StabilityP0Runner()

        # Simulate module with restarts
        runner.results["modules"]["restart-test"] = {
            "name": "restart-test",
            "status": "FAIL",
            "metrics": {
                "restarting-svc": {
                    "stats": {},
                    "inspect": {"restart_count": 2, "oom_killed": False},
                    "health_latency_ms": [100.0],
                }
            },
            "test_results": {},
            "errors": ["Service restarted during test"],
        }

        json_str = json.dumps(runner.results)
        data = json.loads(json_str)

        restart_count = data["modules"]["restart-test"]["metrics"]["restarting-svc"][
            "inspect"
        ]["restart_count"]
        assert restart_count == 2


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
