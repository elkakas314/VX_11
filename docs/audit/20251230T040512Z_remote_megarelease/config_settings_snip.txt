        default_factory=lambda: _resolve_docker_url("hormiguero", 8004)
    )
    manifestator_url: str = Field(
        default_factory=lambda: _resolve_docker_url("manifestator", 8005)
    )
    mcp_url: str = Field(default_factory=lambda: _resolve_docker_url("mcp", 8006))
    shub_url: str = Field(
        default_factory=lambda: _resolve_docker_url("shubniggurath", 8007)
    )
    spawner_url: str = Field(
        default_factory=lambda: _resolve_docker_url("spawner", 8008)
    )
    operator_url: str = Field(
        default_factory=lambda: _resolve_docker_url("operator-backend", 8011)
    )

    # ========== SEGURIDAD ==========
    api_token: str = Field(default="vx11-token-production")
    enable_auth: bool = True
    token_header: str = "X-VX11-Token"
    require_https: bool = False  # En dev; activar en prod
    allowed_origins: list[str] = [
        "http://localhost:3000",
        "http://127.0.0.1:3000",
        "http://localhost:8011",
        "http://localhost:8020",
        "http://127.0.0.1:8011",
        "http://127.0.0.1:8020",
        "http://localhost:5173",
        "http://127.0.0.1:5173",
        "http://localhost:5174",
        "http://127.0.0.1:5174",
    ]
    testing_mode: bool = False  # Si True, puede desactivar auth en tests

    # ========== BASES DE DATOS ==========
    database_path: str = "/app/data/runtime"
    database_url: str = "sqlite:////app/data/runtime/vx11.db"

    # ========== CONFIGURACIÃ“N DE MODELOS IA ==========
    openai_api_key: Optional[str] = None
    deepseek_api_key: Optional[str] = None
    local_model_endpoint: str = "http://localhost:11434"
    default_model: str = "auto"  # auto, local, deepseek

    # ========== HERMES (CLI TOOLS) ==========
    hermes_timeout: int = 30
    max_local_tokens: int = 1000

    # ========== MODELOS LOCALES Y DESCUBRIMIENTO ==========
    local_models_path: str = "/app/models"
    huggingface_cache_path: Optional[str] = "/app/models/.cache"
    max_local_models: int = 20
    max_local_models_gb: int = 2  # Ultra-low-memory: 2 GB max
    model_scan_interval_seconds: int = 300

    # ========== ENDPOINTS LOCALES ==========
    llama_cpp_endpoint: str = "http://localhost:8000"
    ollama_endpoint: str = "http://localhost:11434"
    hf_api_timeout: int = 30

